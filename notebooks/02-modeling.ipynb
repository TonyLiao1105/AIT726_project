{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "project root added to sys.path: c:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\n"
                    ]
                }
            ],
            "source": [
                "import sys, os\n",
                "proj_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # notebooks/ -> project root\n",
                "if proj_root not in sys.path:\n",
                "    sys.path.insert(0, proj_root)\n",
                "print(\"project root added to sys.path:\", proj_root)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "kernel python: c:\\Users\\tonyl\\anaconda3\\python.exe\n",
                        "spaCy OK: 3.8.9\n"
                    ]
                }
            ],
            "source": [
                "import sys, subprocess, importlib\n",
                "\n",
                "print(\"kernel python:\", sys.executable)\n",
                "\n",
                "def ensure(pkg):\n",
                "    try:\n",
                "        importlib.import_module(pkg)\n",
                "    except ModuleNotFoundError:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
                "\n",
                "# install spaCy and small model into this kernel\n",
                "ensure(\"spacy\")\n",
                "# download small English model if missing\n",
                "import spacy\n",
                "try:\n",
                "    spacy.load(\"en_core_web_sm\")\n",
                "except OSError:\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
                "\n",
                "print(\"spaCy OK:\", spacy.__version__)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import typing as t\n",
                "from dataclasses import dataclass\n",
                "import pandas as pd\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "\n",
                "@dataclass\n",
                "class ExampleAnn:\n",
                "    start: int\n",
                "    end: int\n",
                "    label: str\n",
                "\n",
                "class CustomDataset:\n",
                "    \"\"\"\n",
                "    Lightweight dataset wrapper expected by the notebook.\n",
                "    Expects a DataFrame with a text column (default 'text') and\n",
                "    a spans column (default 'spans') where spans is a list of\n",
                "    {\"start\": int, \"end\": int, \"label\": str} or tuples (start,end,label).\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, df: pd.DataFrame, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        self.df = df.reset_index(drop=True)\n",
                "        self.text_col = text_col\n",
                "        self.spans_col = spans_col\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx: int):\n",
                "        row = self.df.iloc[idx]\n",
                "        text = row.get(self.text_col, \"\") if isinstance(row, (pd.Series, dict)) else \"\"\n",
                "        raw_spans = row.get(self.spans_col, []) if isinstance(row, (pd.Series, dict)) else []\n",
                "        \n",
                "        # Handle NaN or non-list spans\n",
                "        if not isinstance(raw_spans, (list, tuple)):\n",
                "            raw_spans = []\n",
                "        \n",
                "        entities: t.List[t.Tuple[int, int, str]] = []\n",
                "        if raw_spans:\n",
                "            for s in raw_spans:\n",
                "                if isinstance(s, dict):\n",
                "                    start = s.get(\"start\")\n",
                "                    end = s.get(\"end\")\n",
                "                    label = s.get(\"label\") or s.get(\"entity\") or s.get(\"label_name\")\n",
                "                elif isinstance(s, (list, tuple)) and len(s) >= 3:\n",
                "                    start, end, label = s[0], s[1], s[2]\n",
                "                else:\n",
                "                    continue\n",
                "                if isinstance(start, int) and isinstance(end, int) and isinstance(label, str):\n",
                "                    entities.append((start, end, label))\n",
                "        return text, {\"entities\": entities}\n",
                "\n",
                "    def to_spacy_docbin(self, nlp: t.Optional[spacy.language.Language] = None) -> DocBin:\n",
                "        \"\"\"Convert dataset to a spaCy DocBin (useful for training).\"\"\"\n",
                "        if nlp is None:\n",
                "            nlp = spacy.blank(\"en\")\n",
                "        db = DocBin()\n",
                "        for i in range(len(self)):\n",
                "            text, ann = self[i]\n",
                "            doc = nlp.make_doc(text)\n",
                "            spans = []\n",
                "            for (start, end, label) in ann.get(\"entities\", []):\n",
                "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
                "                if span is None:\n",
                "                    # skip spans that don't align with tokenization\n",
                "                    continue\n",
                "                spans.append(span)\n",
                "            doc.ents = spans\n",
                "            db.add(doc)\n",
                "        return db\n",
                "\n",
                "    @classmethod\n",
                "    def from_jsonl(cls, path: str, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        \"\"\"Load a DataFrame-backed dataset from a JSONL file (each line a JSON object).\"\"\"\n",
                "        df = pd.read_json(path, lines=True)\n",
                "        return cls(df, text_col=text_col, spans_col=spans_col)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Records after preprocessing: 468\n",
                        "Train size: 374, Val size: 94\n",
                        "Epoch 1/30 - Loss: 2587.8562\n",
                        "Epoch 2/30 - Loss: 1847.9281\n",
                        "Epoch 3/30 - Loss: 1780.9780\n",
                        "Epoch 4/30 - Loss: 1055.6613\n",
                        "Epoch 5/30 - Loss: 818.7966\n",
                        "  VAL (epoch 5) - precision: 0.660 recall: 0.554 f1: 0.602\n",
                        "Epoch 6/30 - Loss: 711.5139\n",
                        "Epoch 7/30 - Loss: 599.9462\n",
                        "Epoch 8/30 - Loss: 514.0048\n",
                        "Epoch 9/30 - Loss: 455.2953\n",
                        "Epoch 10/30 - Loss: 440.0603\n",
                        "  VAL (epoch 10) - precision: 0.730 recall: 0.616 f1: 0.668\n",
                        "Epoch 11/30 - Loss: 346.3121\n",
                        "Epoch 12/30 - Loss: 315.1131\n",
                        "Epoch 13/30 - Loss: 307.4078\n",
                        "Epoch 14/30 - Loss: 278.0275\n",
                        "Epoch 15/30 - Loss: 242.6769\n",
                        "  VAL (epoch 15) - precision: 0.691 recall: 0.589 f1: 0.636\n",
                        "Epoch 16/30 - Loss: 215.0601\n",
                        "Epoch 17/30 - Loss: 240.1756\n",
                        "Epoch 18/30 - Loss: 210.8510\n",
                        "Epoch 19/30 - Loss: 173.8491\n",
                        "Epoch 20/30 - Loss: 174.2090\n",
                        "  VAL (epoch 20) - precision: 0.700 recall: 0.647 f1: 0.673\n",
                        "Epoch 21/30 - Loss: 181.9427\n",
                        "Epoch 22/30 - Loss: 168.7590\n",
                        "Epoch 23/30 - Loss: 158.7034\n",
                        "Epoch 24/30 - Loss: 143.3942\n",
                        "Epoch 25/30 - Loss: 139.7543\n",
                        "  VAL (epoch 25) - precision: 0.734 recall: 0.603 f1: 0.662\n",
                        "Epoch 26/30 - Loss: 132.2067\n",
                        "Epoch 27/30 - Loss: 133.3910\n",
                        "Epoch 28/30 - Loss: 117.1177\n",
                        "Epoch 29/30 - Loss: 100.3512\n",
                        "Epoch 30/30 - Loss: 119.4256\n",
                        "  VAL (epoch 30) - precision: 0.707 recall: 0.625 f1: 0.664\n",
                        "{'precision': 0.7070707070707071, 'recall': 0.625, 'f1': 0.6635071090047393, 'per_label': {'BUS-DEFENDANT': {'precision': 0.8888888888888888, 'recall': 0.4444444444444444, 'f1': 0.5925925925925926, 'tp': 8, 'fp': 1, 'fn': 10}, 'CO-DEFENDANT': {'precision': 1.0, 'recall': 0.06666666666666667, 'f1': 0.125, 'tp': 1, 'fp': 0, 'fn': 14}, 'DEFENDANT': {'precision': 0.6756756756756757, 'recall': 0.625, 'f1': 0.6493506493506493, 'tp': 25, 'fp': 12, 'fn': 15}, 'FRAUD AMOUNT': {'precision': 0.375, 'recall': 0.5, 'f1': 0.42857142857142855, 'tp': 3, 'fp': 5, 'fn': 3}, 'FRAUD MECHANISM': {'precision': 0.375, 'recall': 0.36, 'f1': 0.3673469387755102, 'tp': 9, 'fp': 15, 'fn': 16}, 'GOV PROGRAM': {'precision': 0.6818181818181818, 'recall': 0.6818181818181818, 'f1': 0.6818181818181818, 'tp': 15, 'fp': 7, 'fn': 7}, 'JUDGE': {'precision': 0.8095238095238095, 'recall': 0.8947368421052632, 'f1': 0.8500000000000001, 'tp': 17, 'fp': 4, 'fn': 2}, 'PROSECUTOR': {'precision': 0.8125, 'recall': 0.8478260869565217, 'f1': 0.8297872340425533, 'tp': 39, 'fp': 9, 'fn': 7}, 'SENTENCE': {'precision': 0.8214285714285714, 'recall': 0.696969696969697, 'f1': 0.7540983606557378, 'tp': 23, 'fp': 5, 'fn': 10}}}\n"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "from sklearn.model_selection import train_test_split\n",
                "from src.ner.dataset import CustomDataset\n",
                "from src.ner.model import NERModel\n",
                "from src.ner.train import train_model\n",
                "from src.evaluation import evaluate_model\n",
                "\n",
                "# Load the raw data\n",
                "data_path = '../data/raw/doj_press_releases.jsonl'\n",
                "df = pd.read_json(data_path, lines=True)\n",
                "\n",
                "# Preprocess the data\n",
                "def preprocess_data(df):\n",
                "    # Drop rows with missing text or spans\n",
                "    df = df.dropna(subset=['text', 'spans'])\n",
                "    # Filter out rows where spans is not a list\n",
                "    df = df[df['spans'].apply(lambda x: isinstance(x, list))]\n",
                "    return df.reset_index(drop=True)\n",
                "\n",
                "df = preprocess_data(df)\n",
                "print(f\"Records after preprocessing: {len(df)}\")\n",
                "\n",
                "# Split the data into training and validation sets\n",
                "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "\n",
                "# Create datasets for training\n",
                "train_dataset = CustomDataset(train_df)\n",
                "val_dataset = CustomDataset(val_df)\n",
                "\n",
                "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
                "\n",
                "# Initialize the NER model\n",
                "model = NERModel()\n",
                "\n",
                "# Train the model\n",
                "train_model(model, train_dataset, val_dataset)\n",
                "\n",
                "# Evaluate the model\n",
                "metrics = evaluate_model(model, val_dataset)\n",
                "print(metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modeling Notebook\n",
                "\n",
                "This notebook is dedicated to building and training the NLP model for identifying entities in Department of Justice press releases. The model will be trained on the processed data and evaluated for its performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\models\\\\ner_model'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ...existing code...\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/ner_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model to\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# inspect some false positives / false negatives\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\\src\\ner\\model.py:34\u001b[0m, in \u001b[0;36mNERModel.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save model to disk.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39mto_disk(path)\n",
                        "File \u001b[1;32mc:\\Users\\tonyl\\anaconda3\\Lib\\site-packages\\spacy\\language.py:2156\u001b[0m, in \u001b[0;36mLanguage.to_disk\u001b[1;34m(self, path, exclude)\u001b[0m\n\u001b[0;32m   2154\u001b[0m     serializers[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p, proc\u001b[38;5;241m=\u001b[39mproc: proc\u001b[38;5;241m.\u001b[39mto_disk(p, exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2155\u001b[0m serializers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mto_disk(p, exclude\u001b[38;5;241m=\u001b[39mexclude)\n\u001b[1;32m-> 2156\u001b[0m util\u001b[38;5;241m.\u001b[39mto_disk(path, serializers, exclude)\n",
                        "File \u001b[1;32mc:\\Users\\tonyl\\anaconda3\\Lib\\site-packages\\spacy\\util.py:1431\u001b[0m, in \u001b[0;36mto_disk\u001b[1;34m(path, writers, exclude)\u001b[0m\n\u001b[0;32m   1429\u001b[0m path \u001b[38;5;241m=\u001b[39m ensure_path(path)\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m-> 1431\u001b[0m     path\u001b[38;5;241m.\u001b[39mmkdir()\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, writer \u001b[38;5;129;01min\u001b[39;00m writers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# Split to support file names like meta.json\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude:\n",
                        "File \u001b[1;32mc:\\Users\\tonyl\\anaconda3\\Lib\\pathlib\\_local.py:722\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[1;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 722\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
                        "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '..\\\\models\\\\ner_model'"
                    ]
                }
            ],
            "source": [
                "# ...existing code...\n",
                "# save model\n",
                "model_path = '../models/ner_model'\n",
                "model.save(model_path)\n",
                "print(\"Saved model to\", model_path)\n",
                "\n",
                "# inspect some false positives / false negatives\n",
                "def inspect_errors(model, dataset, n=10):\n",
                "    cnt = 0\n",
                "    for i in range(len(dataset)):\n",
                "        text, ann = dataset[i]\n",
                "        true_set = set((s, e, l) for (s, e, l) in ann.get(\"entities\", []))\n",
                "        preds = model.predict(text)\n",
                "        pred_set = set((start, end, label) for (_, label, start, end) in preds)\n",
                "        fp = pred_set - true_set\n",
                "        fn = true_set - pred_set\n",
                "        if fp or fn:\n",
                "            print(\"---- example\", i, \"----\")\n",
                "            print(text)\n",
                "            if fp:\n",
                "                print(\"  False Positives:\")\n",
                "                for s,e,l in fp:\n",
                "                    print(f\"    {l}: '{text[s:e]}' ({s}-{e})\")\n",
                "            if fn:\n",
                "                print(\"  False Negatives:\")\n",
                "                for s,e,l in fn:\n",
                "                    print(f\"    {l}: '{text[s:e]}' ({s}-{e})\")\n",
                "            cnt += 1\n",
                "            if cnt >= n:\n",
                "                break\n",
                "\n",
                "inspect_errors(model, val_dataset, n=10)\n",
                "# ...existing code..."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "project root added to sys.path: c:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\n"
                    ]
                }
            ],
            "source": [
                "import sys, os\n",
                "proj_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # notebooks/ -> project root\n",
                "if proj_root not in sys.path:\n",
                "    sys.path.insert(0, proj_root)\n",
                "print(\"project root added to sys.path:\", proj_root)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "kernel python: c:\\Users\\tonyl\\anaconda3\\python.exe\n",
                        "spaCy OK: 3.8.9\n"
                    ]
                }
            ],
            "source": [
                "import sys, subprocess, importlib\n",
                "\n",
                "print(\"kernel python:\", sys.executable)\n",
                "\n",
                "def ensure(pkg):\n",
                "    try:\n",
                "        importlib.import_module(pkg)\n",
                "    except ModuleNotFoundError:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
                "\n",
                "# install spaCy and small model into this kernel\n",
                "ensure(\"spacy\")\n",
                "# download small English model if missing\n",
                "import spacy\n",
                "try:\n",
                "    spacy.load(\"en_core_web_sm\")\n",
                "except OSError:\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
                "\n",
                "print(\"spaCy OK:\", spacy.__version__)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import typing as t\n",
                "from dataclasses import dataclass\n",
                "import pandas as pd\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "\n",
                "@dataclass\n",
                "class ExampleAnn:\n",
                "    start: int\n",
                "    end: int\n",
                "    label: str\n",
                "\n",
                "class CustomDataset:\n",
                "    \"\"\"\n",
                "    Lightweight dataset wrapper expected by the notebook.\n",
                "    Expects a DataFrame with a text column (default 'text') and\n",
                "    a spans column (default 'spans') where spans is a list of\n",
                "    {\"start\": int, \"end\": int, \"label\": str} or tuples (start,end,label).\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, df: pd.DataFrame, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        self.df = df.reset_index(drop=True)\n",
                "        self.text_col = text_col\n",
                "        self.spans_col = spans_col\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx: int):\n",
                "        row = self.df.iloc[idx]\n",
                "        text = row.get(self.text_col, \"\") if isinstance(row, (pd.Series, dict)) else \"\"\n",
                "        raw_spans = row.get(self.spans_col, []) if isinstance(row, (pd.Series, dict)) else []\n",
                "        \n",
                "        # Handle NaN or non-list spans\n",
                "        if not isinstance(raw_spans, (list, tuple)):\n",
                "            raw_spans = []\n",
                "        \n",
                "        entities: t.List[t.Tuple[int, int, str]] = []\n",
                "        if raw_spans:\n",
                "            for s in raw_spans:\n",
                "                if isinstance(s, dict):\n",
                "                    start = s.get(\"start\")\n",
                "                    end = s.get(\"end\")\n",
                "                    label = s.get(\"label\") or s.get(\"entity\") or s.get(\"label_name\")\n",
                "                elif isinstance(s, (list, tuple)) and len(s) >= 3:\n",
                "                    start, end, label = s[0], s[1], s[2]\n",
                "                else:\n",
                "                    continue\n",
                "                if isinstance(start, int) and isinstance(end, int) and isinstance(label, str):\n",
                "                    entities.append((start, end, label))\n",
                "        return text, {\"entities\": entities}\n",
                "\n",
                "    def to_spacy_docbin(self, nlp: t.Optional[spacy.language.Language] = None) -> DocBin:\n",
                "        \"\"\"Convert dataset to a spaCy DocBin (useful for training).\"\"\"\n",
                "        if nlp is None:\n",
                "            nlp = spacy.blank(\"en\")\n",
                "        db = DocBin()\n",
                "        for i in range(len(self)):\n",
                "            text, ann = self[i]\n",
                "            doc = nlp.make_doc(text)\n",
                "            spans = []\n",
                "            for (start, end, label) in ann.get(\"entities\", []):\n",
                "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
                "                if span is None:\n",
                "                    # skip spans that don't align with tokenization\n",
                "                    continue\n",
                "                spans.append(span)\n",
                "            doc.ents = spans\n",
                "            db.add(doc)\n",
                "        return db\n",
                "\n",
                "    @classmethod\n",
                "    def from_jsonl(cls, path: str, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        \"\"\"Load a DataFrame-backed dataset from a JSONL file (each line a JSON object).\"\"\"\n",
                "        df = pd.read_json(path, lines=True)\n",
                "        return cls(df, text_col=text_col, spans_col=spans_col)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Records after preprocessing: 468\n",
                        "Train size: 374, Val size: 94\n",
                        "Epoch 1/10 - Loss: 3439.9531\n",
                        "Epoch 2/10 - Loss: 2349.2573\n",
                        "Epoch 3/10 - Loss: 1894.6348\n",
                        "Epoch 4/10 - Loss: 1621.7538\n",
                        "Epoch 5/10 - Loss: 1491.2950\n",
                        "Epoch 6/10 - Loss: 1342.3236\n",
                        "Epoch 7/10 - Loss: 1238.6257\n",
                        "Epoch 8/10 - Loss: 1249.5037\n",
                        "Epoch 9/10 - Loss: 1082.3400\n",
                        "Epoch 10/10 - Loss: 1056.1290\n",
                        "{'precision': 0.1618045846162823, 'recall': 0.13392857142857142, 'f1': 0.14368536269428422}\n"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "from sklearn.model_selection import train_test_split\n",
                "from src.ner.dataset import CustomDataset\n",
                "from src.ner.model import NERModel\n",
                "from src.ner.train import train_model\n",
                "from src.evaluation import evaluate_model\n",
                "\n",
                "# Load the raw data\n",
                "data_path = '../data/raw/doj_press_releases.jsonl'\n",
                "df = pd.read_json(data_path, lines=True)\n",
                "\n",
                "# Preprocess the data\n",
                "def preprocess_data(df):\n",
                "    # Drop rows with missing text or spans\n",
                "    df = df.dropna(subset=['text', 'spans'])\n",
                "    # Filter out rows where spans is not a list\n",
                "    df = df[df['spans'].apply(lambda x: isinstance(x, list))]\n",
                "    return df.reset_index(drop=True)\n",
                "\n",
                "df = preprocess_data(df)\n",
                "print(f\"Records after preprocessing: {len(df)}\")\n",
                "\n",
                "# Split the data into training and validation sets\n",
                "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "\n",
                "# Create datasets for training\n",
                "train_dataset = CustomDataset(train_df)\n",
                "val_dataset = CustomDataset(val_df)\n",
                "\n",
                "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
                "\n",
                "# Initialize the NER model\n",
                "model = NERModel()\n",
                "\n",
                "# Train the model\n",
                "train_model(model, train_dataset, val_dataset)\n",
                "\n",
                "# Evaluate the model\n",
                "metrics = evaluate_model(model, val_dataset)\n",
                "print(metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modeling Notebook\n",
                "\n",
                "This notebook is dedicated to building and training the NLP model for identifying entities in Department of Justice press releases. The model will be trained on the processed data and evaluated for its performance."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

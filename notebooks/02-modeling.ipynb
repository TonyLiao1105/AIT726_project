{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "project root added to sys.path: c:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\n"
                    ]
                }
            ],
            "source": [
                "import sys, os\n",
                "proj_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # notebooks/ -> project root\n",
                "if proj_root not in sys.path:\n",
                "    sys.path.insert(0, proj_root)\n",
                "print(\"project root added to sys.path:\", proj_root)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "kernel python: c:\\Users\\tonyl\\anaconda3\\python.exe\n",
                        "spaCy OK: 3.8.9\n"
                    ]
                }
            ],
            "source": [
                "# ...existing code...\n",
                "import sys, subprocess, importlib\n",
                "\n",
                "print(\"kernel python:\", sys.executable)\n",
                "\n",
                "def ensure(pkg):\n",
                "    try:\n",
                "        importlib.import_module(pkg)\n",
                "    except ModuleNotFoundError:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
                "\n",
                "# install spaCy and small model into this kernel\n",
                "ensure(\"spacy\")\n",
                "# download small English model if missing\n",
                "import spacy\n",
                "try:\n",
                "    spacy.load(\"en_core_web_sm\")\n",
                "except OSError:\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
                "\n",
                "print(\"spaCy OK:\", spacy.__version__)\n",
                "# ...existing code..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ...existing code...\n",
                "import typing as t\n",
                "from dataclasses import dataclass\n",
                "import pandas as pd\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "\n",
                "@dataclass\n",
                "class ExampleAnn:\n",
                "    start: int\n",
                "    end: int\n",
                "    label: str\n",
                "\n",
                "class CustomDataset:\n",
                "    \"\"\"\n",
                "    Lightweight dataset wrapper expected by the notebook.\n",
                "    Expects a DataFrame with a text column (default 'text') and\n",
                "    a spans column (default 'spans') where spans is a list of\n",
                "    {\"start\": int, \"end\": int, \"label\": str} or tuples (start,end,label).\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, df: pd.DataFrame, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        self.df = df.reset_index(drop=True)\n",
                "        self.text_col = text_col\n",
                "        self.spans_col = spans_col\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx: int):\n",
                "        row = self.df.iloc[idx]\n",
                "        text = row.get(self.text_col, \"\") if isinstance(row, (pd.Series, dict)) else \"\"\n",
                "        raw_spans = row.get(self.spans_col, []) if isinstance(row, (pd.Series, dict)) else []\n",
                "        \n",
                "        # Handle NaN or non-list spans\n",
                "        if not isinstance(raw_spans, (list, tuple)):\n",
                "            raw_spans = []\n",
                "        \n",
                "        entities: t.List[t.Tuple[int, int, str]] = []\n",
                "        if raw_spans:\n",
                "            for s in raw_spans:\n",
                "                if isinstance(s, dict):\n",
                "                    start = s.get(\"start\")\n",
                "                    end = s.get(\"end\")\n",
                "                    label = s.get(\"label\") or s.get(\"entity\") or s.get(\"label_name\")\n",
                "                elif isinstance(s, (list, tuple)) and len(s) >= 3:\n",
                "                    start, end, label = s[0], s[1], s[2]\n",
                "                else:\n",
                "                    continue\n",
                "                if isinstance(start, int) and isinstance(end, int) and isinstance(label, str):\n",
                "                    entities.append((start, end, label))\n",
                "        return text, {\"entities\": entities}\n",
                "\n",
                "    def to_spacy_docbin(self, nlp: t.Optional[spacy.language.Language] = None) -> DocBin:\n",
                "        \"\"\"Convert dataset to a spaCy DocBin (useful for training).\"\"\"\n",
                "        if nlp is None:\n",
                "            nlp = spacy.blank(\"en\")\n",
                "        db = DocBin()\n",
                "        for i in range(len(self)):\n",
                "            text, ann = self[i]\n",
                "            doc = nlp.make_doc(text)\n",
                "            spans = []\n",
                "            for (start, end, label) in ann.get(\"entities\", []):\n",
                "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
                "                if span is None:\n",
                "                    # skip spans that don't align with tokenization\n",
                "                    continue\n",
                "                spans.append(span)\n",
                "            doc.ents = spans\n",
                "            db.add(doc)\n",
                "        return db\n",
                "\n",
                "    @classmethod\n",
                "    def from_jsonl(cls, path: str, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        \"\"\"Load a DataFrame-backed dataset from a JSONL file (each line a JSON object).\"\"\"\n",
                "        df = pd.read_json(path, lines=True)\n",
                "        return cls(df, text_col=text_col, spans_col=spans_col)\n",
                "# ...existing code..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "language": "python"
            },
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "'float' object is not iterable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[7], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m NERModel()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m train_model(model, train_dataset, val_dataset)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_dataset)\n",
                        "File \u001b[1;32mc:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\\src\\ner\\train.py:10\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataset, val_dataset, epochs, drop)\u001b[0m\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset)):\n\u001b[1;32m---> 10\u001b[0m     _, ann \u001b[38;5;241m=\u001b[39m train_dataset[i]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start, end, label \u001b[38;5;129;01min\u001b[39;00m ann\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n\u001b[0;32m     12\u001b[0m         labels\u001b[38;5;241m.\u001b[39madd(label)\n",
                        "File \u001b[1;32mc:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\\src\\ner\\dataset.py:36\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     34\u001b[0m entities: t\u001b[38;5;241m.\u001b[39mList[t\u001b[38;5;241m.\u001b[39mTuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_spans:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m raw_spans:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     38\u001b[0m             start \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "from sklearn.model_selection import train_test_split\n",
                "from src.ner.dataset import CustomDataset\n",
                "from src.ner.model import NERModel\n",
                "from src.ner.train import train_model\n",
                "from src.evaluation import evaluate_model\n",
                "\n",
                "# Load the raw data\n",
                "data_path = '../data/raw/doj_press_releases.jsonl'\n",
                "df = pd.read_json(data_path, lines=True)\n",
                "\n",
                "# Preprocess the data\n",
                "def preprocess_data(df):\n",
                "    # Implement preprocessing steps such as cleaning and tokenization\n",
                "    return df\n",
                "\n",
                "df = preprocess_data(df)\n",
                "\n",
                "# Split the data into training and validation sets\n",
                "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "\n",
                "# Create datasets for training\n",
                "train_dataset = CustomDataset(train_df)\n",
                "val_dataset = CustomDataset(val_df)\n",
                "\n",
                "# Initialize the NER model\n",
                "model = NERModel()\n",
                "\n",
                "# Train the model\n",
                "train_model(model, train_dataset, val_dataset)\n",
                "\n",
                "# Evaluate the model\n",
                "metrics = evaluate_model(model, val_dataset)\n",
                "print(metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modeling Notebook\n",
                "\n",
                "This notebook is dedicated to building and training the NLP model for identifying entities in Department of Justice press releases. The model will be trained on the processed data and evaluated for its performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "kernel python: c:\\Users\\tonyl\\AppData\\Local\\Programs\\Python\\Python314\\python.exe\n"
                    ]
                }
            ],
            "source": [
                "# ...existing code...\n",
                "# filepath: c:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\\notebooks\\02-modeling.ipynb\n",
                "import sys\n",
                "print(\"kernel python:\", sys.executable)\n",
                "# ...existing code..."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

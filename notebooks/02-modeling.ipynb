{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "project root added to sys.path: c:\\Tony\\GMU\\AIT 726\\Project\\doj-press-release-nlp\n"
                    ]
                }
            ],
            "source": [
                "import sys, os\n",
                "proj_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # notebooks/ -> project root\n",
                "if proj_root not in sys.path:\n",
                "    sys.path.insert(0, proj_root)\n",
                "print(\"project root added to sys.path:\", proj_root)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "kernel python: c:\\Users\\tonyl\\anaconda3\\python.exe\n",
                        "spaCy OK: 3.8.9\n"
                    ]
                }
            ],
            "source": [
                "import sys, subprocess, importlib\n",
                "\n",
                "print(\"kernel python:\", sys.executable)\n",
                "\n",
                "def ensure(pkg):\n",
                "    try:\n",
                "        importlib.import_module(pkg)\n",
                "    except ModuleNotFoundError:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
                "\n",
                "# install spaCy and small model into this kernel\n",
                "ensure(\"spacy\")\n",
                "# download small English model if missing\n",
                "import spacy\n",
                "try:\n",
                "    spacy.load(\"en_core_web_sm\")\n",
                "except OSError:\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
                "\n",
                "print(\"spaCy OK:\", spacy.__version__)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import typing as t\n",
                "from dataclasses import dataclass\n",
                "import pandas as pd\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "\n",
                "@dataclass\n",
                "class ExampleAnn:\n",
                "    start: int\n",
                "    end: int\n",
                "    label: str\n",
                "\n",
                "class CustomDataset:\n",
                "    \"\"\"\n",
                "    Lightweight dataset wrapper expected by the notebook.\n",
                "    Expects a DataFrame with a text column (default 'text') and\n",
                "    a spans column (default 'spans') where spans is a list of\n",
                "    {\"start\": int, \"end\": int, \"label\": str} or tuples (start,end,label).\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, df: pd.DataFrame, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        self.df = df.reset_index(drop=True)\n",
                "        self.text_col = text_col\n",
                "        self.spans_col = spans_col\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx: int):\n",
                "        row = self.df.iloc[idx]\n",
                "        text = row.get(self.text_col, \"\") if isinstance(row, (pd.Series, dict)) else \"\"\n",
                "        raw_spans = row.get(self.spans_col, []) if isinstance(row, (pd.Series, dict)) else []\n",
                "        \n",
                "        # Handle NaN or non-list spans\n",
                "        if not isinstance(raw_spans, (list, tuple)):\n",
                "            raw_spans = []\n",
                "        \n",
                "        entities: t.List[t.Tuple[int, int, str]] = []\n",
                "        if raw_spans:\n",
                "            for s in raw_spans:\n",
                "                if isinstance(s, dict):\n",
                "                    start = s.get(\"start\")\n",
                "                    end = s.get(\"end\")\n",
                "                    label = s.get(\"label\") or s.get(\"entity\") or s.get(\"label_name\")\n",
                "                elif isinstance(s, (list, tuple)) and len(s) >= 3:\n",
                "                    start, end, label = s[0], s[1], s[2]\n",
                "                else:\n",
                "                    continue\n",
                "                if isinstance(start, int) and isinstance(end, int) and isinstance(label, str):\n",
                "                    entities.append((start, end, label))\n",
                "        return text, {\"entities\": entities}\n",
                "\n",
                "    def to_spacy_docbin(self, nlp: t.Optional[spacy.language.Language] = None) -> DocBin:\n",
                "        \"\"\"Convert dataset to a spaCy DocBin (useful for training).\"\"\"\n",
                "        if nlp is None:\n",
                "            nlp = spacy.blank(\"en\")\n",
                "        db = DocBin()\n",
                "        for i in range(len(self)):\n",
                "            text, ann = self[i]\n",
                "            doc = nlp.make_doc(text)\n",
                "            spans = []\n",
                "            for (start, end, label) in ann.get(\"entities\", []):\n",
                "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
                "                if span is None:\n",
                "                    # skip spans that don't align with tokenization\n",
                "                    continue\n",
                "                spans.append(span)\n",
                "            doc.ents = spans\n",
                "            db.add(doc)\n",
                "        return db\n",
                "\n",
                "    @classmethod\n",
                "    def from_jsonl(cls, path: str, text_col: str = \"text\", spans_col: str = \"spans\"):\n",
                "        \"\"\"Load a DataFrame-backed dataset from a JSONL file (each line a JSON object).\"\"\"\n",
                "        df = pd.read_json(path, lines=True)\n",
                "        return cls(df, text_col=text_col, spans_col=spans_col)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Records after preprocessing: 468\n",
                        "Train size: 374, Val size: 94\n",
                        "Epoch 1/30 - Loss: 2345.7568\n",
                        "Epoch 2/30 - Loss: 2209.0701\n",
                        "Epoch 3/30 - Loss: 2188.6702\n",
                        "Epoch 4/30 - Loss: 1303.8115\n",
                        "Epoch 5/30 - Loss: 798.9683\n",
                        "  VAL (epoch 5) - precision: 0.609 recall: 0.598 f1: 0.604\n",
                        "Epoch 6/30 - Loss: 685.4832\n",
                        "Epoch 7/30 - Loss: 609.9827\n",
                        "Epoch 8/30 - Loss: 548.9823\n",
                        "Epoch 9/30 - Loss: 444.8132\n",
                        "Epoch 10/30 - Loss: 418.9741\n",
                        "  VAL (epoch 10) - precision: 0.664 recall: 0.634 f1: 0.648\n",
                        "Epoch 11/30 - Loss: 387.1442\n",
                        "Epoch 12/30 - Loss: 304.2717\n",
                        "Epoch 13/30 - Loss: 288.9045\n",
                        "Epoch 14/30 - Loss: 278.3461\n",
                        "Epoch 15/30 - Loss: 262.9724\n",
                        "  VAL (epoch 15) - precision: 0.654 recall: 0.634 f1: 0.644\n",
                        "Epoch 16/30 - Loss: 228.2742\n",
                        "Epoch 17/30 - Loss: 222.3318\n",
                        "Epoch 18/30 - Loss: 194.6564\n",
                        "Epoch 19/30 - Loss: 198.2361\n",
                        "Epoch 20/30 - Loss: 176.1124\n",
                        "  VAL (epoch 20) - precision: 0.656 recall: 0.562 f1: 0.606\n",
                        "Epoch 21/30 - Loss: 186.7772\n",
                        "Epoch 22/30 - Loss: 167.1012\n",
                        "Epoch 23/30 - Loss: 157.0625\n",
                        "Epoch 24/30 - Loss: 152.6126\n",
                        "Epoch 25/30 - Loss: 150.7372\n",
                        "  VAL (epoch 25) - precision: 0.693 recall: 0.616 f1: 0.652\n",
                        "Epoch 26/30 - Loss: 163.9206\n",
                        "Epoch 27/30 - Loss: 139.3932\n",
                        "Epoch 28/30 - Loss: 135.6297\n",
                        "Epoch 29/30 - Loss: 116.4070\n",
                        "Epoch 30/30 - Loss: 122.2533\n",
                        "  VAL (epoch 30) - precision: 0.647 recall: 0.580 f1: 0.612\n",
                        "{'precision': 0.6467661691542289, 'recall': 0.5803571428571429, 'f1': 0.611764705882353, 'per_label': {'BUS-DEFENDANT': {'precision': 0.875, 'recall': 0.3888888888888889, 'f1': 0.5384615384615385, 'tp': 7, 'fp': 1, 'fn': 11}, 'CO-DEFENDANT': {'precision': 0.3333333333333333, 'recall': 0.06666666666666667, 'f1': 0.1111111111111111, 'tp': 1, 'fp': 2, 'fn': 14}, 'DEFENDANT': {'precision': 0.5714285714285714, 'recall': 0.5, 'f1': 0.5333333333333333, 'tp': 20, 'fp': 15, 'fn': 20}, 'FRAUD AMOUNT': {'precision': 0.25, 'recall': 0.6666666666666666, 'f1': 0.36363636363636365, 'tp': 4, 'fp': 12, 'fn': 2}, 'FRAUD MECHANISM': {'precision': 0.34782608695652173, 'recall': 0.32, 'f1': 0.3333333333333333, 'tp': 8, 'fp': 15, 'fn': 17}, 'GOV PROGRAM': {'precision': 0.5769230769230769, 'recall': 0.6818181818181818, 'f1': 0.6249999999999999, 'tp': 15, 'fp': 11, 'fn': 7}, 'JUDGE': {'precision': 0.8571428571428571, 'recall': 0.9473684210526315, 'f1': 0.9, 'tp': 18, 'fp': 3, 'fn': 1}, 'PROSECUTOR': {'precision': 0.8461538461538461, 'recall': 0.717391304347826, 'f1': 0.776470588235294, 'tp': 33, 'fp': 6, 'fn': 13}, 'SENTENCE': {'precision': 0.8, 'recall': 0.7272727272727273, 'f1': 0.761904761904762, 'tp': 24, 'fp': 6, 'fn': 9}}}\n"
                    ]
                }
            ],
            "source": [
                "# Import necessary libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import spacy\n",
                "from spacy.tokens import DocBin\n",
                "from sklearn.model_selection import train_test_split\n",
                "from src.ner.dataset import CustomDataset\n",
                "from src.ner.model import NERModel\n",
                "from src.ner.train import train_model\n",
                "from src.evaluation import evaluate_model\n",
                "\n",
                "# Load the raw data\n",
                "data_path = '../data/raw/doj_press_releases.jsonl'\n",
                "df = pd.read_json(data_path, lines=True)\n",
                "\n",
                "# Preprocess the data\n",
                "def preprocess_data(df):\n",
                "    # Drop rows with missing text or spans\n",
                "    df = df.dropna(subset=['text', 'spans'])\n",
                "    # Filter out rows where spans is not a list\n",
                "    df = df[df['spans'].apply(lambda x: isinstance(x, list))]\n",
                "    return df.reset_index(drop=True)\n",
                "\n",
                "df = preprocess_data(df)\n",
                "print(f\"Records after preprocessing: {len(df)}\")\n",
                "\n",
                "# Split the data into training and validation sets\n",
                "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "\n",
                "# Create datasets for training\n",
                "train_dataset = CustomDataset(train_df)\n",
                "val_dataset = CustomDataset(val_df)\n",
                "\n",
                "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
                "\n",
                "# Initialize the NER model\n",
                "model = NERModel()\n",
                "\n",
                "# Train the model\n",
                "train_model(model, train_dataset, val_dataset)\n",
                "\n",
                "# Evaluate the model\n",
                "metrics = evaluate_model(model, val_dataset)\n",
                "print(metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modeling Notebook\n",
                "\n",
                "This notebook is dedicated to building and training the NLP model for identifying entities in Department of Justice press releases. The model will be trained on the processed data and evaluated for its performance."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
